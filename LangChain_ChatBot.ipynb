{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zJ_NA50U88uy",
        "dAg1H0DT6LAs",
        "8e49HSSqCfmc",
        "FxVLj6m9Cngr",
        "NjQCABmWC0kV",
        "_UHbgC9GC9Bv",
        "GzEHAFcbDKi1",
        "nve4vtjsDdbZ",
        "wv7_zGOzD4W8",
        "lnkp-516Fqpi"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openAI_API = userdata.get('API_KEY')"
      ],
      "metadata": {
        "id": "iU7STocDHx8C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# basic chat implementation"
      ],
      "metadata": {
        "id": "tgvWSj5_7P2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain langchain-openai langchain-chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8ABoB9snDt--",
        "outputId": "3a3f321f-d144-4954-c0a8-916035164579"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.2/121.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiU70HI6pydk",
        "outputId": "36c1b0b5-673b-4786-ae08-893fe747c8cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.2,openai_api_key=openAI_API)"
      ],
      "metadata": {
        "id": "rENqmn0rDwxo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we invoke our chat model, the output is an AIMessage:"
      ],
      "metadata": {
        "id": "WLr0K4urrVyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "chat.invoke(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=\"Translate this sentence from English to Telugu: I love programming.\"\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AfINWIvo_bJ",
        "outputId": "6fb19cba-383c-4535-f9cc-6dfe8552e7f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='నేను ప్రోగ్రమింగ్ నేర్చుకున్నాను. (Nēnu prōgramiṅg nērcukunnānu)', response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 20, 'total_tokens': 101}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0e50e68a-5a16-4523-acce-9c8cd44e8655-0')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model on its own does not have any concept of state. For example, if you ask a followup question:"
      ],
      "metadata": {
        "id": "V-5IgX77rSqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat.invoke([HumanMessage(content=\"What did you just say?\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXl4Uyz-prs9",
        "outputId": "f7d371b9-7e9c-48e9-9110-805d3de25da6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I apologize if I said something unclear. Can you please let me know what you would like me to clarify or repeat?', response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 13, 'total_tokens': 37}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-60d252a4-bb9c-488e-a6b7-e9adcdbd4b1a-0')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it doesn't take the previous conversation turn into context, and cannot answer the question.\n",
        "\n",
        "To get around this, we need to pass the entire conversation history into the model. Let's see what happens when we do that:"
      ],
      "metadata": {
        "id": "FN4gt1fprN1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "chat.invoke(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=\"Translate this sentence from English to Telugu: I love programming.\"\n",
        "        ),\n",
        "        AIMessage(content=\"నాకు ప్రోగ్రమింగ్ నచ్చు.\"),\n",
        "        HumanMessage(content=\"What did you just say?\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKQXxapUqgIb",
        "outputId": "b62630ca-7599-45a3-ad98-1bc40b292444"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I said \"నాకు ప్రోగ్రమింగ్ నచ్చు\" which means \"I love programming\" in Telugu.', response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 77, 'total_tokens': 134}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-031a83f1-faf2-499a-a0b7-03cfa1d1f321-0')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt templates**\n",
        "\n",
        "Let's define a prompt template to make formatting a bit easier. We can create a chain by piping it into the model:"
      ],
      "metadata": {
        "id": "pMrHA0-Yq_KM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat"
      ],
      "metadata": {
        "id": "daf1yDZOqve8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MessagesPlaceholder above inserts chat messages passed into the chain's input as chat_history directly into the prompt. Then, we can invoke the chain like this:\n",
        "\n"
      ],
      "metadata": {
        "id": "irsfVl8QrHUT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wj6nNZn6rICw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Message history**\n",
        "\n",
        "As a shortcut for managing the chat history, we can use a MessageHistory class, which is responsible for saving and loading chat messages.\n",
        "\n",
        "There are many built-in message history integrations that persist messages to a variety of databases, but for this quickstart we'll use a in-memory, demo message history called ChatMessageHistory.\n",
        "\n"
      ],
      "metadata": {
        "id": "eYXdZJFFrmX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "demo_ephemeral_chat_history = ChatMessageHistory()\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(\"hi!\")\n",
        "\n",
        "demo_ephemeral_chat_history.add_ai_message(\"whats up?\")\n",
        "demo_ephemeral_chat_history.add_user_message(\"nothing\")\n",
        "demo_ephemeral_chat_history.add_ai_message(\"Ok Good bye\")\n",
        "\n",
        "demo_ephemeral_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PBzSMamrqup",
        "outputId": "4e73132d-2222-4b1f-ea1b-0aa0a8f7cd7f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='hi!'),\n",
              " AIMessage(content='whats up?'),\n",
              " HumanMessage(content='nothing'),\n",
              " AIMessage(content='Ok Good bye')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history.add_user_message(\n",
        "    \"Translate this sentence from English to French: I love programming.\"\n",
        ")\n",
        "\n",
        "response = chain.invoke({\"messages\": demo_ephemeral_chat_history.messages})\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UnAK0e8rtS-",
        "outputId": "06e7b33a-0dfb-4b55-8d1f-1488a7f522e8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The translation of \"I love programming\" in French is \"J\\'adore la programmation.\"', response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 65, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a49e5623-9faa-44c0-8dd4-2e876c5cc511-0')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history.add_ai_message(response)\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(\"What did you just say?\")\n",
        "\n",
        "chain.invoke({\"messages\": demo_ephemeral_chat_history.messages})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X03RGNw6sFdy",
        "outputId": "f1f7dd14-992a-4c21-b94c-bc5d780b7937"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I said the translation of \"I love programming\" in French is \"J\\'adore la programmation.\"', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 99, 'total_tokens': 121}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-af23e13a-59be-46ef-90ed-9a7cd8aab8d2-0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uL1GZu0KsO4M"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrievers**\n",
        "\n",
        "We can set up and use a Retriever to pull domain-specific knowledge for our chatbot. To show this, let's expand the simple chatbot we created above to be able to answer questions about Document"
      ],
      "metadata": {
        "id": "_pRXBIoJ0kEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain-chroma beautifulsoup4"
      ],
      "metadata": {
        "id": "VBzU3zAY0otF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "-au8ZoAk0tf2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " we split it into smaller chunks that the LLM's context window can handle and store it in a vector database:"
      ],
      "metadata": {
        "id": "HmSOVkBl04Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10)\n",
        "all_splits = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "DQz_cNLB0zjK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we embed and store those chunks in a vector database:"
      ],
      "metadata": {
        "id": "eYheOHga1CqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=all_splits,\n",
        "    embedding=OpenAIEmbeddings(openai_api_key=openAI_API)\n",
        ")\n"
      ],
      "metadata": {
        "id": "zQ-ZK_iC06us"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieval"
      ],
      "metadata": {
        "id": "_Uyi13rN-IcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, let's create a retriever from our initialized vectorstore:"
      ],
      "metadata": {
        "id": "Ft8NROMZ2FRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# k is the number of chunks to retrieve\n",
        "retriever = vectorstore.as_retriever(k=4)\n",
        "\n",
        "docs = retriever.invoke(\"What is langsmith?\")\n",
        "\n",
        "docs[0].metadata[\"description\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Fo5JmAEV1HlI",
        "outputId": "ea979784-4174-4ae3-e6a3-83ac816a3f27"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(k=10)\n",
        "\n",
        "docs = retriever.invoke(\"How langsmith is used for testing?\")\n",
        "\n",
        "docs[0].metadata[\"description\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "uP6XI5ms2PAL",
        "outputId": "e7351d44-57d3-4aeb-8759-89c54ecde653"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hWxgFiJg22eo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling documents**\n",
        "\n",
        "Let's modify our previous prompt to accept documents as context. We'll use a create_stuff_documents_chain helper function to \"stuff\" all of the input documents into the prompt, which also conveniently handles formatting. We use the ChatPromptTemplate.from_messages method to format the message input we want to pass to the model, including a MessagesPlaceholder where chat history messages will be directly injected:"
      ],
      "metadata": {
        "id": "pVGbQem03IWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\",openai_api_key=openAI_API)\n",
        "\n",
        "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Answer the user's questions based on the below context:\\n\\n{context}\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "document_chain = create_stuff_documents_chain(chat, question_answering_prompt)"
      ],
      "metadata": {
        "id": "_kJz9Blv3IyW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "demo_ephemeral_chat_history = ChatMessageHistory()\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(\"how can langsmith help with testing?\")\n",
        "\n",
        "document_chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history.messages,\n",
        "        \"context\": docs,\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "eU11hwKS3YBY",
        "outputId": "affcc1a2-eb2d-4a28-d701-9bfaa19df7db"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"LangSmith is a platform that allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. It provides the tools and features for building production-grade LLM applications. With LangSmith, you can log traces of your application's behavior and performance, and then use the evaluation tools to analyze and assess the performance of your application. This can be very helpful for testing as it allows you to closely monitor and evaluate your application's behavior, identify any issues or bottlenecks, and make improvements as needed. Additionally, LangSmith does not require the use of LangChain, so you can use it on its own for testing and evaluation purposes.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "demo_ephemeral_chat_history = ChatMessageHistory()\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(\"what are the usecases of langsmith?\")\n",
        "\n",
        "document_chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history.messages,\n",
        "        \"context\": docs,\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "N7jRmzIM3iuL",
        "outputId": "28e4f44b-2100-4802-bdd8-ef6dcdf95d13"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangSmith is a platform designed for building production-grade LLM (Language Model) applications. It is particularly useful for closely monitoring and evaluating your application, allowing you to ship quickly and with confidence. Some potential use cases for LangSmith include:\\n\\n1. Developing and deploying language-based applications\\n2. Monitoring and evaluating the performance of language models\\n3. Building and testing natural language processing (NLP) applications\\n4. Ensuring the quality and accuracy of language-based AI applications\\n5. Streamlining the development and deployment process for language models and applications\\n\\nOverall, LangSmith is geared towards empowering developers to create and manage robust and efficient language-based applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BgnWUXds3uWp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a retrieval chain**\n",
        "\n",
        "Next, let's integrate our retriever into the chain. Our retriever should retrieve information relevant to the last message we pass in from the user, so we extract it and use that as input to fetch relevant docs, which we add to the current chain as context. We pass context plus the previous messages into our document chain to generate a final answer.\n",
        "\n",
        "We also use the RunnablePassthrough.assign() method to pass intermediate steps through at each invocation. Here's what it looks like:"
      ],
      "metadata": {
        "id": "Vs7OJswq3-Tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "def parse_retriever_input(params: Dict):\n",
        "    return params[\"messages\"][-1].content\n",
        "\n",
        "\n",
        "retrieval_chain = RunnablePassthrough.assign(\n",
        "    context=parse_retriever_input | retriever,\n",
        ").assign(\n",
        "    answer=document_chain,\n",
        ")"
      ],
      "metadata": {
        "id": "zFvn2duh4CTj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history.messages,\n",
        "    }\n",
        ")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRtGTua34Eei",
        "outputId": "2a8fd175-182e-4bdf-e47b-33fe25a1d6f7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='what are the usecases of langsmith?')],\n",
              " 'context': [Document(page_content='Skip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith‚ÄãPythonTypeScriptpip install -U langsmithyarn add langchain', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content=\"langchain langsmith2. Create an API key‚ÄãTo create an API key head to the Settings page. Then click Create API Key.3. Set up your environment‚ÄãShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>4. Log your first trace‚ÄãWe provide multiple ways to log traces to LangSmith. Below, we'll highlight\", metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='metadata: {      version: \"1.0.0\",      revision_id: \"beta\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2024 LangChain, Inc.', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n",
              " 'answer': 'LangSmith is a platform designed for building production-grade LLM (Language Model) applications. Some of the use cases for LangSmith include:\\n\\n1. Language Model Development: LangSmith can be used to develop and monitor language models for various applications, such as chatbots, language translation, content generation, and more.\\n\\n2. Application Monitoring: It allows for closely monitoring and evaluating your application, enabling you to ship quickly and with confidence.\\n\\n3. Trace Logging and Evaluation: LangSmith provides ways to log traces and run evaluations, which is useful for tracking the performance and behavior of language models in real-world scenarios.\\n\\nOverall, LangSmith is suitable for developers and organizations looking to build and deploy language model applications while ensuring performance and reliability.'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history.add_ai_message(response[\"answer\"])\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(\"tell me more about that!\")\n",
        "\n",
        "retrieval_chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history.messages,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhZ7mtdn4IkO",
        "outputId": "52ae91f0-fae6-47bc-fa77-0dfc1625b559"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='what are the usecases of langsmith?'),\n",
              "  AIMessage(content='LangSmith is a platform designed for building production-grade LLM (Language Model) applications. Some of the use cases for LangSmith include:\\n\\n1. Language Model Development: LangSmith can be used to develop and monitor language models for various applications, such as chatbots, language translation, content generation, and more.\\n\\n2. Application Monitoring: It allows for closely monitoring and evaluating your application, enabling you to ship quickly and with confidence.\\n\\n3. Trace Logging and Evaluation: LangSmith provides ways to log traces and run evaluations, which is useful for tracking the performance and behavior of language models in real-world scenarios.\\n\\nOverall, LangSmith is suitable for developers and organizations looking to build and deploy language model applications while ensuring performance and reliability.'),\n",
              "  HumanMessage(content='tell me more about that!')],\n",
              " 'context': [Document(page_content='Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='metadata: {      version: \"1.0.0\",      revision_id: \"beta\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2024 LangChain, Inc.', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content=\"langchain langsmith2. Create an API key‚ÄãTo create an API key head to the Settings page. Then click Create API Key.3. Set up your environment‚ÄãShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>4. Log your first trace‚ÄãWe provide multiple ways to log traces to LangSmith. Below, we'll highlight\", metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='content: user_input }],        model: \"gpt-3.5-turbo\",    });    return result.choices[0].message.content;});await pipeline(\"Hello, world!\")// Out: Hello there! How can I assist you today?View a sample output trace.Learn more about tracing in the how-to guides.5. Run your first evaluation‚ÄãEvaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator.PythonTypeScriptfrom langsmith import Clientfrom', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n",
              " 'answer': \"LangSmith provides a comprehensive set of tools and features to support the development and deployment of language model applications. Here are some key aspects of LangSmith:\\n\\n1. Trace Logging: LangSmith offers multiple ways to log traces, allowing developers to monitor and analyze the behavior of language models in real-world scenarios. This feature helps in identifying and addressing any issues or inconsistencies in the model's performance.\\n\\n2. Evaluation: The platform supports running evaluations to test the language model's performance, using built-in accuracy evaluators and test cases. This ensures that the model is functioning as expected and can be used confidently in production environments.\\n\\n3. API Key Management: LangSmith allows users to create and manage API keys, providing secure access to the platform's features and services.\\n\\n4. Language Model Integration: LangSmith seamlessly integrates with various language models, such as GPT-3.5 Turbo, enabling developers to leverage powerful language generation capabilities in their applications.\\n\\n5. Community Support: LangSmith fosters community engagement through its Discord, Twitter, and GitHub channels, where developers can connect, collaborate, and learn from each other.\\n\\nBy offering these capabilities, LangSmith empowers developers and organizations to build, test, and deploy robust language model applications with confidence and efficiency.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pbbi8WR54Nud"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! Our chatbot can now answer domain-specific questions in a conversational way.\n",
        "\n",
        "As an aside, if you don't want to return all the intermediate steps, you can define your retrieval chain like this using a pipe directly into the document chain instead of the final .assign() call:\n",
        "\n"
      ],
      "metadata": {
        "id": "1QMJ9ktp4WlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_chain_with_only_answer = (\n",
        "    RunnablePassthrough.assign(\n",
        "        context=parse_retriever_input | retriever,\n",
        "    )\n",
        "    | document_chain\n",
        ")\n",
        "\n",
        "retrieval_chain_with_only_answer.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history.messages,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "b_sF8o7t4XiP",
        "outputId": "c2c96065-98ff-495c-8ea5-f98084913eb2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"LangSmith offers a comprehensive set of tools and features to support the development and deployment of language model applications. Here are some key aspects to consider:\\n\\n1. Trace Logging: LangSmith provides multiple ways to log traces, allowing you to capture and analyze the behavior and performance of your language models in real-world usage scenarios.\\n\\n2. Evaluation Framework: The platform includes built-in evaluators to grade the results of language model tests, enabling you to assess the accuracy and effectiveness of your models.\\n\\n3. API Key Management: LangSmith allows you to create and manage API keys, providing secure access to the platform's resources and services.\\n\\n4. Environment Setup: The platform offers guidance on setting up your development environment, including the use of environment variables and integration with other API services, such as the OpenAI API.\\n\\n5. Language Support: LangSmith supports multiple programming languages, including Python and JavaScript/TypeScript, making it accessible to a wide range of developers and applications.\\n\\nOverall, LangSmith is designed to streamline the development, monitoring, and evaluation of language model applications, empowering developers to create robust and high-performing language-based solutions.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sgrJdfs34Y4Z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query transformation\n"
      ],
      "metadata": {
        "id": "PGGI-cnn44Oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableBranch\n",
        "\n",
        "# We need a prompt that we can pass into an LLM to generate a transformed search query\n",
        "\n",
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.2,openai_api_key=openAI_API)\n",
        "\n",
        "query_transform_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        (\n",
        "            \"user\",\n",
        "            \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "query_transforming_retriever_chain = RunnableBranch(\n",
        "    (\n",
        "        lambda x: len(x.get(\"messages\", [])) == 1,\n",
        "        # If only one message, then we just pass that message's content to retriever\n",
        "        (lambda x: x[\"messages\"][-1].content) | retriever,\n",
        "    ),\n",
        "    # If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever\n",
        "    query_transform_prompt | chat | StrOutputParser() | retriever,\n",
        ").with_config(run_name=\"chat_retriever_chain\")"
      ],
      "metadata": {
        "id": "XYWzdQIC45LT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_chain = create_stuff_documents_chain(chat, question_answering_prompt)\n",
        "\n",
        "conversational_retrieval_chain = RunnablePassthrough.assign(\n",
        "    context=query_transforming_retriever_chain,\n",
        ").assign(\n",
        "    answer=document_chain,\n",
        ")\n",
        "\n",
        "demo_ephemeral_chat_history = ChatMessageHistory()"
      ],
      "metadata": {
        "id": "BhaNKQpe45ps"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history.add_user_message(\"how can langsmith help with testing?\")\n",
        "\n",
        "response = conversational_retrieval_chain.invoke(\n",
        "    {\"messages\": demo_ephemeral_chat_history.messages},\n",
        ")\n",
        "\n",
        "demo_ephemeral_chat_history.add_ai_message(response[\"answer\"])\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_OYoRfr5Kaf",
        "outputId": "5045883f-bb64-41e1-f5a0-83f2c9ba6afa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='how can langsmith help with testing?'),\n",
              "  AIMessage(content='LangSmith is a platform designed to help you closely monitor and evaluate your application, allowing you to build production-grade LLM (Language Model) applications with confidence. It provides tools and features to help you test and evaluate your application, so you can ship quickly and with confidence. LangSmith offers the ability to log traces, create API keys, and set up your environment for testing and evaluation. Additionally, it provides how-to guides and tutorials to help you learn more about evaluation and testing with LangSmith. Overall, LangSmith is a valuable tool for testing and monitoring the performance of your LLM applications.')],\n",
              " 'context': [Document(page_content='Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='Skip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith‚ÄãPythonTypeScriptpip install -U langsmithyarn add langchain', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='metadata: {      version: \"1.0.0\",      revision_id: \"beta\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2024 LangChain, Inc.', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content=\"langchain langsmith2. Create an API key‚ÄãTo create an API key head to the Settings page. Then click Create API Key.3. Set up your environment‚ÄãShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>4. Log your first trace‚ÄãWe provide multiple ways to log traces to LangSmith. Below, we'll highlight\", metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n",
              " 'answer': 'LangSmith is a platform designed to help you closely monitor and evaluate your application, allowing you to build production-grade LLM (Language Model) applications with confidence. It provides tools and features to help you test and evaluate your application, so you can ship quickly and with confidence. LangSmith offers the ability to log traces, create API keys, and set up your environment for testing and evaluation. Additionally, it provides how-to guides and tutorials to help you learn more about evaluation and testing with LangSmith. Overall, LangSmith is a valuable tool for testing and monitoring the performance of your LLM applications.'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history.add_user_message(\"tell me more about that!\")\n",
        "\n",
        "conversational_retrieval_chain.invoke(\n",
        "    {\"messages\": demo_ephemeral_chat_history.messages}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soGTjnkx5Lz1",
        "outputId": "8991cb8d-447b-463d-91b1-18731f06f0f6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='how can langsmith help with testing?'),\n",
              "  AIMessage(content='LangSmith is a platform designed to help you closely monitor and evaluate your application, allowing you to build production-grade LLM (Language Model) applications with confidence. It provides tools and features to help you test and evaluate your application, so you can ship quickly and with confidence. LangSmith offers the ability to log traces, create API keys, and set up your environment for testing and evaluation. Additionally, it provides how-to guides and tutorials to help you learn more about evaluation and testing with LangSmith. Overall, LangSmith is a valuable tool for testing and monitoring the performance of your LLM applications.'),\n",
              "  HumanMessage(content='tell me more about that!')],\n",
              " 'context': [Document(page_content='Skip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith‚ÄãPythonTypeScriptpip install -U langsmithyarn add langchain', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='metadata: {      version: \"1.0.0\",      revision_id: \"beta\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2024 LangChain, Inc.', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content=\"langchain langsmith2. Create an API key‚ÄãTo create an API key head to the Settings page. Then click Create API Key.3. Set up your environment‚ÄãShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>4. Log your first trace‚ÄãWe provide multiple ways to log traces to LangSmith. Below, we'll highlight\", metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n",
              " 'answer': \"LangSmith is a platform that offers a range of features to support testing and evaluation of production-grade LLM applications. Here are some key aspects of LangSmith that can help with testing:\\n\\n1. Monitoring and Evaluation: LangSmith allows you to closely monitor and evaluate your application's performance, providing insights into how your LLM application is functioning in real-world scenarios.\\n\\n2. Tracing and Logging: LangSmith provides multiple ways to log traces, allowing you to track the behavior and performance of your application over time. This can be valuable for testing and debugging purposes.\\n\\n3. API Key Management: LangSmith enables you to create API keys, which can be used to securely access and interact with the LangSmith platform for testing and evaluation purposes.\\n\\n4. Environment Setup: LangSmith offers guidance on setting up your environment for testing, including the use of environment variables and integration with other APIs such as OpenAI.\\n\\n5. How-to Guides and Tutorials: LangSmith provides resources such as how-to guides and tutorials to help you learn more about evaluation, testing, and best practices for building and testing LLM applications.\\n\\nOverall, LangSmith is a comprehensive platform that supports testing and evaluation, providing the tools and resources needed to ensure the reliability and performance of your LLM applications.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory management"
      ],
      "metadata": {
        "id": "v58y0jZq7IaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Memory to Store\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "thUmmie453T0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Neo4j"
      ],
      "metadata": {
        "id": "zJ_NA50U88uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install neo4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XPbqbBK45-Jm",
        "outputId": "ed0d3622-a9fc-4024-8376-f6d618b9c55f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neo4j\n",
            "  Downloading neo4j-5.20.0.tar.gz (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.0/203.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2023.4)\n",
            "Building wheels for collected packages: neo4j\n",
            "  Building wheel for neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neo4j: filename=neo4j-5.20.0-py3-none-any.whl size=280771 sha256=79beb827ac7be4bab86dabcbfc0f4d8cd8c71573b0f5fb09be3931a41b1d7180\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/12/66/764554d079caad4b9a11a02cfc0d200dd876d12935b9cf7e64\n",
            "Successfully built neo4j\n",
            "Installing collected packages: neo4j\n",
            "Successfully installed neo4j-5.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import Neo4jChatMessageHistory\n",
        "\n",
        "history = Neo4jChatMessageHistory(\n",
        "    url=\"bolt://localhost:7687\",\n",
        "    username=\"neo4j\",\n",
        "    password=\"password\",\n",
        "    session_id=\"session_id_1\",\n",
        ")\n",
        "\n",
        "history.add_user_message(\"hi!\")\n",
        "\n",
        "history.add_ai_message(\"whats up?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "collapsed": true,
        "id": "GZnHYtAC54gY",
        "outputId": "0288e557-812b-4f30-acdc-1d545062b61e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Could not connect to Neo4j database. Please ensure that the url is correct",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_async_compat/network/_bolt_socket.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(cls, resolved_address, timeout, keep_alive)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[#0000]  C: <OPEN> %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolved_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m             \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mServiceUnavailable\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_async_compat/network/_bolt_socket.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(cls, address, tcp_timeout, deadline, custom_resolver, ssl_context, keep_alive)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m                 s = BoltSocket._connect(resolved_address, tcp_timeout,\n\u001b[0m\u001b[1;32m    691\u001b[0m                                         keep_alive)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_async_compat/network/_bolt_socket.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(cls, resolved_address, timeout, keep_alive)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                 raise ServiceUnavailable(\n\u001b[0m\u001b[1;32m    547\u001b[0m                     \u001b[0;34m\"Failed to establish connection to {!r} (reason {})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mServiceUnavailable\u001b[0m: Failed to establish connection to ResolvedIPv4Address(('127.0.0.1', 7687)) (reason [Errno 111] Connection refused)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mServiceUnavailable\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/chat_message_histories/neo4j.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_id, url, username, password, database, node_label, window, graph)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_connectivity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mneo4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServiceUnavailable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/driver.py\u001b[0m in \u001b[0;36mverify_connectivity\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \u001b[0msession_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_session_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_server_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/driver.py\u001b[0m in \u001b[0;36m_get_server_info\u001b[0;34m(self, session_config)\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_server_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/work/session.py\u001b[0m in \u001b[0;36m_get_server_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAD_ACCESS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mliveness_check_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mserver_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/work/session.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(self, access_mode, **acquire_kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             super()._connect(\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0maccess_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0macquire_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/work/workspace.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(self, access_mode, auth, **acquire_kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0macquire_kwargs_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macquire_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0macquire_kwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection_access_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccess_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/io/_pool.py\u001b[0m in \u001b[0;36macquire\u001b[0;34m(self, access_mode, timeout, database, bookmarks, auth, liveness_check_timeout)\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mdeadline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeadline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_timeout_or_deadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         return self._acquire(\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mliveness_check_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/io/_pool.py\u001b[0m in \u001b[0;36m_acquire\u001b[0;34m(self, address, auth, deadline, liveness_check_timeout)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[#0000]  _: <POOL> trying to hand out new connection\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/io/_pool.py\u001b[0m in \u001b[0;36mconnection_creator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                     connection = self.opener(\n\u001b[0m\u001b[1;32m    164\u001b[0m                         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/io/_pool.py\u001b[0m in \u001b[0;36mopener\u001b[0;34m(addr, auth_manager, deadline)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m             return Bolt.open(\n\u001b[0m\u001b[1;32m    501\u001b[0m                 \u001b[0maddr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/io/_bolt.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, address, auth_manager, deadline, routing_context, pool_config)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandshake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             BoltSocket.connect(\n\u001b[0m\u001b[1;32m    402\u001b[0m                 \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_async_compat/network/_bolt_socket.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(cls, address, tcp_timeout, deadline, custom_resolver, ssl_context, keep_alive)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             raise ServiceUnavailable(\n\u001b[0m\u001b[1;32m    719\u001b[0m                 \"Couldn't connect to %s (resolved to %s):\\n%s\" % (\n",
            "\u001b[0;31mServiceUnavailable\u001b[0m: Couldn't connect to localhost:7687 (resolved to ()):\nFailed to establish connection to ResolvedIPv4Address(('127.0.0.1', 7687)) (reason [Errno 111] Connection refused)\nFailed to establish connection to ResolvedIPv6Address(('::1', 7687, 0, 0)) (reason [Errno 99] Cannot assign requested address)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-2c98c3bf4ae4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_message_histories\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeo4jChatMessageHistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = Neo4jChatMessageHistory(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bolt://localhost:7687\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0musername\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"neo4j\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/chat_message_histories/neo4j.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_id, url, username, password, database, node_label, window, graph)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_connectivity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mneo4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServiceUnavailable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0;34m\"Could not connect to Neo4j database. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0;34m\"Please ensure that the url is correct\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Could not connect to Neo4j database. Please ensure that the url is correct"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.messages"
      ],
      "metadata": {
        "id": "oKoE0o9758eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stremlit"
      ],
      "metadata": {
        "id": "dAg1H0DT6LAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install streamlit"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eNJvQjxg6QIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import (\n",
        "    StreamlitChatMessageHistory,\n",
        ")\n",
        "\n",
        "history = StreamlitChatMessageHistory(key=\"chat_messages\")\n",
        "\n",
        "history.add_user_message(\"hi!\")\n",
        "history.add_ai_message(\"whats up?\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KiX7XG-y6NYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rqWIYBCz6tpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h8b5haWR6tnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MmmE3kad6tk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##We can use it directly to store conversation turns for our chain:\n",
        "\n"
      ],
      "metadata": {
        "id": "8vpi7wWI6t_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history = ChatMessageHistory()\n",
        "\n",
        "input1 = \"Translate this sentence from English to French: I love programming.\"\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(input1)\n",
        "\n",
        "response = chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history.messages,\n",
        "    }\n",
        ")\n",
        "\n",
        "demo_ephemeral_chat_history.add_ai_message(response)\n",
        "\n",
        "input2 = \"What did I just ask you?\"\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(input2)\n",
        "\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history.messages,\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRdtljGu6wEt",
        "outputId": "44fc73c2-1d1c-45de-a8ad-a021d7d6fa2a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='You just asked me to translate the sentence \"I love programming\" from English to French.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 74, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3c05bfff-69b0-409d-a40b-1292a428ca77-0')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Automatic history management**\n",
        "\n",
        "The previous examples pass messages to the chain explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also includes an wrapper for LCEL chains that can handle this process automatically called ***RunnableWithMessageHistory.***\n",
        "\n",
        "To show how it works, let's slightly modify the above prompt to take a final input variable that populates a HumanMessage template after the chat history. This means that we will expect a chat_history parameter that contains all messages BEFORE the current messages instead of all messages:\n",
        "\n"
      ],
      "metadata": {
        "id": "DoTsO28Q6zpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat"
      ],
      "metadata": {
        "id": "2kkL5W9i6wl0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll pass the latest input to the conversation here and let the RunnableWithMessageHistory class wrap our chain and do the work of appending that input variable to the chat history."
      ],
      "metadata": {
        "id": "u-PcdiMF6_RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "demo_ephemeral_chat_history_for_chain = ChatMessageHistory()\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: demo_ephemeral_chat_history_for_chain,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")"
      ],
      "metadata": {
        "id": "1I3zlSPY67g1"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class takes a few parameters in addition to the chain that we want to wrap:\n",
        "\n",
        "A factory function that returns a message history for a given session id. This allows your chain to handle multiple users at once by loading different messages for different conversations.\n",
        "An input_messages_key that specifies which part of the input should be tracked and stored in the chat history. In this example, we want to track the string passed in as input.\n",
        "\n",
        "A history_messages_key that specifies what the previous messages should be injected into the prompt as. Our prompt has a MessagesPlaceholder named chat_history, so we specify this property to match.\n",
        "\n",
        "(For chains with multiple outputs) an output_messages_key which specifies which output to store as history. This is the inverse of input_messages_key.\n",
        "\n",
        "We can invoke this new chain as normal, with an additional configurable field that specifies the particular session_id to pass to the factory function. This is unused for the demo, but in real-world chains, you'll want to return a chat history corresponding to the passed session:"
      ],
      "metadata": {
        "id": "BJW-20mz7lfW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FhvGa6zM7A0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modifying chat history**\n",
        "\n",
        "Modifying stored chat messages can help your chatbot handle a variety of situations. Here are some examples:\n",
        "\n"
      ],
      "metadata": {
        "id": "0k66WeIL7yem"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P9d2QIzn70mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Trimming messages**\n",
        "\n",
        "LLMs and chat models have limited context windows, and even if you're not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is to only load and store the most recent n messages. Let's use an example history with some preloaded messages:"
      ],
      "metadata": {
        "id": "ip5E3JpJ708p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history = ChatMessageHistory()\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(\"Hey there! I'm Nemo.\")\n",
        "demo_ephemeral_chat_history.add_ai_message(\"Hello!\")\n",
        "demo_ephemeral_chat_history.add_user_message(\"How are you today?\")\n",
        "demo_ephemeral_chat_history.add_ai_message(\"Fine thanks!\")\n",
        "\n",
        "demo_ephemeral_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHRsMR8b8DFT",
        "outputId": "eb71018d-a0bb-44fa-ff9a-4f8a1322690c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
              " AIMessage(content='Hello!'),\n",
              " HumanMessage(content='How are you today?'),\n",
              " AIMessage(content='Fine thanks!')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: demo_ephemeral_chat_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")\n",
        "\n",
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"What's my name?\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW5ah1Tz76NH",
        "outputId": "0fc426ca-b54b-4f24-89ca-72fa64c9e14d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.tracers.base:Parent run b5d282c5-02e0-4595-9908-6a0d9e5d00fb not found for run 10f50f1f-c3f0-4995-8a99-378d6355ea30. Treating as a root run.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Nemo.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 66, 'total_tokens': 72}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5313c842-178f-4a18-a0c5-0bb069742a6f-0')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the chain remembers the preloaded name.\n",
        "\n",
        "But let's say we have a very small context window, and we want to trim the number of messages passed to the chain to only the 2 most recent ones. We can use the clear method to remove messages and re-add them to the history. We don't have to, but let's put this method at the front of our chain to ensure it's always called:\n",
        "\n"
      ],
      "metadata": {
        "id": "jzTvBTnp8G6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trim_messages(chain_input):\n",
        "    stored_messages = demo_ephemeral_chat_history.messages\n",
        "    if len(stored_messages) <= 2:\n",
        "        return False\n",
        "\n",
        "    demo_ephemeral_chat_history.clear()\n",
        "\n",
        "    for message in stored_messages[-2:]:\n",
        "        demo_ephemeral_chat_history.add_message(message)\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "chain_with_trimming = (\n",
        "    RunnablePassthrough.assign(messages_trimmed=trim_messages)\n",
        "    | chain_with_message_history\n",
        ")"
      ],
      "metadata": {
        "id": "dltANxu87_OP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's call this new chain and check the messages afterwards:"
      ],
      "metadata": {
        "id": "ULuiVAOi8WsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_trimming.invoke(\n",
        "    {\"input\": \"Where does P. Sherman live?\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIAEmnft8Mj2",
        "outputId": "c8a3fc1e-fc72-41f2-aeb6-b734ddfe243b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.tracers.base:Parent run ddeb1139-0fb4-49a2-821e-195b9d438962 not found for run 94414444-3233-4f9a-a824-d8300b0fcfae. Treating as a root run.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='P. Sherman is a fictional character from the movie \"Finding Nemo,\" and he lives at 42 Wallaby Way, Sydney.', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 53, 'total_tokens': 80}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6f59435c-866d-4bef-b685-1789661432ed-0')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXcAgij38UvW",
        "outputId": "98d39c87-93e5-4ef3-8032-002066339aa5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"What's my name?\"),\n",
              " AIMessage(content='Your name is Nemo.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 66, 'total_tokens': 72}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5313c842-178f-4a18-a0c5-0bb069742a6f-0'),\n",
              " HumanMessage(content='Where does P. Sherman live?'),\n",
              " AIMessage(content='P. Sherman is a fictional character from the movie \"Finding Nemo,\" and he lives at 42 Wallaby Way, Sydney.', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 53, 'total_tokens': 80}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6f59435c-866d-4bef-b685-1789661432ed-0')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7lSLQuJF8Yge"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can see that our history has removed the two oldest messages while still adding the most recent conversation at the end. The next time the chain is called, trim_messages will be called again, and only the two most recent messages will be passed to the model. In this case, this means that the model will forget the name we gave it the next time we invoke it:\n",
        "\n"
      ],
      "metadata": {
        "id": "_VAEDvYb8dGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_trimming.invoke(\n",
        "    {\"input\": \"What is my name?\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRjtXvwX8dgA",
        "outputId": "f598d962-ada5-4525-9f01-861bbb7823fe"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.tracers.base:Parent run 77a30f1f-7bf1-4142-ab61-eeb9809eee37 not found for run fe47d5ad-20c3-405e-8211-6c6d0cecfa67. Treating as a root run.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, I don't have access to your personal information. Therefore, I don't know your name.\", response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 74, 'total_tokens': 97}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ca74b513-b13b-4478-ac34-07686a8a9069-0')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WTb6ywW8e0a",
        "outputId": "f2f72cc4-edcb-4f06-f415-e881fe30e481"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Where does P. Sherman live?'),\n",
              " AIMessage(content='P. Sherman is a fictional character from the movie \"Finding Nemo,\" and he lives at 42 Wallaby Way, Sydney.', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 53, 'total_tokens': 80}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6f59435c-866d-4bef-b685-1789661432ed-0'),\n",
              " HumanMessage(content='What is my name?'),\n",
              " AIMessage(content=\"I'm sorry, I don't have access to your personal information. Therefore, I don't know your name.\", response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 74, 'total_tokens': 97}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ca74b513-b13b-4478-ac34-07686a8a9069-0')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FSL95-pZ8gle"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Summary memory**\n",
        "\n",
        "We can use this same pattern in other ways too. For example, we could use an additional LLM call to generate a summary of the conversation before calling our chain. Let's recreate our chat history and chatbot chain:"
      ],
      "metadata": {
        "id": "PQbvliwT8jS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_messages(chain_input):\n",
        "    stored_messages = demo_ephemeral_chat_history.messages\n",
        "    if len(stored_messages) == 0:\n",
        "        return False\n",
        "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\n",
        "                \"user\",\n",
        "                \"Distill the above chat messages into a single summary message. Include as many specific details as you can.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    summarization_chain = summarization_prompt | chat\n",
        "\n",
        "    summary_message = summarization_chain.invoke({\"chat_history\": stored_messages})\n",
        "\n",
        "    demo_ephemeral_chat_history.clear()\n",
        "\n",
        "    demo_ephemeral_chat_history.add_message(summary_message)\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "chain_with_summarization = (\n",
        "    RunnablePassthrough.assign(messages_summarized=summarize_messages)\n",
        "    | chain_with_message_history\n",
        ")"
      ],
      "metadata": {
        "id": "PwrxcvPD8k9U"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_summarization.invoke(\n",
        "    {\"input\": \"What did I say my name was?\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkRwmllu8sYo",
        "outputId": "949f886f-6e85-4fed-caf3-29dbd0def3ac"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.tracers.base:Parent run 651a2f23-d459-4335-8eeb-7304cdd218a1 not found for run ed348657-8a26-4030-a6c7-f34977f35307. Treating as a root run.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, but I don't have access to personal information, so I don't know your name. If you mentioned your name earlier in the conversation, I don't have the ability to remember it. How can I assist you today?\", response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 96, 'total_tokens': 145}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bd4d8d15-e87b-418e-91be-81452441e03f-0')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erniIX7N8uEq",
        "outputId": "ef949349-9d65-40a3-9455-bfa9988d4c16"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='The chat discussed the whereabouts of P. Sherman, a fictional character from the movie \"Finding Nemo,\" who lives at 42 Wallaby Way, Sydney. Additionally, it mentioned that the assistant does not have access to personal information, so it does not know the user\\'s name.', response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 106, 'total_tokens': 163}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-63822236-c584-403c-9faf-773ec123927b-0'),\n",
              " HumanMessage(content='What did I say my name was?'),\n",
              " AIMessage(content=\"I'm sorry, but I don't have access to personal information, so I don't know your name. If you mentioned your name earlier in the conversation, I don't have the ability to remember it. How can I assist you today?\", response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 96, 'total_tokens': 145}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bd4d8d15-e87b-418e-91be-81452441e03f-0')]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0agwOTaLAMSz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tool Calling"
      ],
      "metadata": {
        "id": "hX9K9i09BiHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the term \"tool calling\" interchangeably with \"function calling\". Although function calling is sometimes meant to refer to invocations of a single function, we treat all models as though they can return multiple tool or function calls in each message."
      ],
      "metadata": {
        "id": "cb3Tc4uKBqdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tool calling allows a model to respond to a given prompt by generating output that matches a user-defined schema. While the name implies that the model is performing some action, this is actually not the case! The model is coming up with the arguments to a tool, and actually running the tool (or not) is up to the user - for example, if you want to extract output matching some schema from unstructured text, you could give the model an \"extraction\" tool that takes parameters matching the desired schema, then treat the generated output as your final result."
      ],
      "metadata": {
        "id": "bgCijaMOByNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tool call includes a name, arguments dict, and an optional identifier. The arguments dict is structured\n",
        "\n",
        "***{argument_name: argument_value}.***"
      ],
      "metadata": {
        "id": "7MdAKRsLB1mI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many LLM providers, including Anthropic, Cohere, Google, Mistral, OpenAI, and others, support variants of a tool calling feature.\n",
        "\n",
        "These features typically allow requests to the LLM to include available tools and their schemas, and for responses to include calls to these tools.\n",
        "\n",
        "*For instance, given a search engine tool, an LLM might handle a query by first issuing a call to the search engine.*\n",
        "\n",
        "The system calling the LLM can receive the tool call, execute it, and return the output to the LLM to inform its response. LangChain includes a suite of built-in tools and supports several methods for defining your own custom tools. Tool-calling is extremely useful for building tool-using chains and agents, and for getting structured outputs from models more generally."
      ],
      "metadata": {
        "id": "zUJZVOt7CGkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Providers adopt different conventions for formatting tool schemas and tool calls."
      ],
      "metadata": {
        "id": "sDpHRJVQCdam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Anthropic"
      ],
      "metadata": {
        "id": "8e49HSSqCfmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance, Anthropic returns tool calls as parsed structures within a larger content block:"
      ],
      "metadata": {
        "id": "C4dPI9hSCinr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[\n",
        "  {\n",
        "    \"text\": \"<thinking>\\nI should use a tool.\\n</thinking>\",\n",
        "    \"type\": \"text\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"id_value\",\n",
        "    \"input\": {\"arg_name\": \"arg_value\"},\n",
        "    \"name\": \"tool_name\",\n",
        "    \"type\": \"tool_use\"\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHHORALDCk1i",
        "outputId": "3828363c-373f-4052-b796-2bada79a21ff"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': '<thinking>\\nI should use a tool.\\n</thinking>', 'type': 'text'},\n",
              " {'id': 'id_value',\n",
              "  'input': {'arg_name': 'arg_value'},\n",
              "  'name': 'tool_name',\n",
              "  'type': 'tool_use'}]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##OpenAI"
      ],
      "metadata": {
        "id": "FxVLj6m9Cngr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "whereas OpenAI separates tool calls into a distinct parameter, with arguments as JSON strings:"
      ],
      "metadata": {
        "id": "vko8tLg2Cph0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"tool_calls\": [\n",
        "    {\n",
        "      \"id\": \"id_value\",\n",
        "      \"function\": {\n",
        "        \"arguments\": '{\"arg_name\": \"arg_value\"}',\n",
        "        \"name\": \"tool_name\"\n",
        "      },\n",
        "      \"type\": \"function\"\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRmgc0AACrZo",
        "outputId": "5daba0e4-7836-4e72-f835-cd1fc6a15846"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tool_calls': [{'id': 'id_value',\n",
              "   'function': {'arguments': '{\"arg_name\": \"arg_value\"}', 'name': 'tool_name'},\n",
              "   'type': 'function'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Request: Passing tools to model"
      ],
      "metadata": {
        "id": "NjQCABmWC0kV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a model to be able to invoke tools, you need to pass tool schemas to it when making a chat request. LangChain ChatModels supporting tool calling features implement a .bind_tools method, which receives a list of LangChain tool objects, Pydantic classes, or JSON Schemas and binds them to the chat model in the provider-specific expected format. Subsequent invocations of the bound chat model will include tool schemas in every call to the model API."
      ],
      "metadata": {
        "id": "AdHeekKiC6rN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Defining tool schemas: LangChain Tool**\n",
        "\n",
        "For example, we can define the schema for custom tools using the **@tool** decorator on Python functions:"
      ],
      "metadata": {
        "id": "_UHbgC9GC9Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiplies a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "tools = [add, multiply]"
      ],
      "metadata": {
        "id": "rZb2MctaDGpn"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining tool schemas: Pydantic class\n",
        "We can equivalently define the schema using Pydantic. Pydantic is useful when your tool inputs are more complex:"
      ],
      "metadata": {
        "id": "GzEHAFcbDKi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rcyf3boxDFeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "# Note that the docstrings here are crucial, as they will be passed along\n",
        "# to the model along with the class name.\n",
        "class add(BaseModel):\n",
        "    \"\"\"Add two integers together.\"\"\"\n",
        "\n",
        "    a: int = Field(..., description=\"First integer\")\n",
        "    b: int = Field(..., description=\"Second integer\")\n",
        "\n",
        "\n",
        "class multiply(BaseModel):\n",
        "    \"\"\"Multiply two integers together.\"\"\"\n",
        "\n",
        "    a: int = Field(..., description=\"First integer\")\n",
        "    b: int = Field(..., description=\"Second integer\")\n",
        "\n",
        "\n",
        "tools = [add, multiply]"
      ],
      "metadata": {
        "id": "8xWiLr4VBjqf"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ec_mcTGfCkV1"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EXTSsU_eDcsV"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Request: Forcing a tool call**\n",
        "\n"
      ],
      "metadata": {
        "id": "nve4vtjsDdbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you just use bind_tools(tools), the model can choose whether to return one tool call, multiple tool calls, or no tool calls at all. Some models support a tool_choice parameter that gives you some ability to force the model to call a tool. For models that support this, you can pass in the name of the tool you want the model to always call tool_choice=\"xyz_tool_name\". Or you can pass in tool_choice=\"any\" to force the model to call at least one tool, without specifying which tool specifically.\n"
      ],
      "metadata": {
        "id": "M974Nt6UDlGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# always_multiply_llm = llm.bind_tools([multiply], tool_choice=\"multiply\")"
      ],
      "metadata": {
        "id": "4ekGVInYDd4H"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# always_multiply_llm = llm.bind_tools([multiply], tool_choice=\"any\")"
      ],
      "metadata": {
        "id": "BEusWF2HDoxv"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lMhB5QZZDsB3"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Response: Reading tool calls from model output\n",
        "\n"
      ],
      "metadata": {
        "id": "wv7_zGOzD4W8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If tool calls are included in a LLM response, they are attached to the corresponding AIMessage or AIMessageChunk (when streaming) as a list of ToolCall objects in the .tool_calls attribute. A ToolCall is a typed dict that includes a tool name, dict of argument values, and (optionally) an identifier. Messages with no tool calls default to an empty list for this attribute.\n"
      ],
      "metadata": {
        "id": "r7VoN01LD7Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
        "\n",
        "# llm_with_tools.invoke(query).tool_calls"
      ],
      "metadata": {
        "id": "5WVkur8TD6iX"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The .tool_calls attribute should contain valid tool calls. Note that on occasion, model providers may output malformed tool calls (e.g., arguments that are not valid JSON). When parsing fails in these cases, instances of InvalidToolCall are populated in the .invalid_tool_calls attribute. An InvalidToolCall can have a name, string arguments, identifier, and error message.\n",
        "\n",
        "If desired, output parsers can further process the output. For example, we can convert back to the original Pydantic class:\n",
        "\n"
      ],
      "metadata": {
        "id": "iBSeYEhoEKZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
        "\n",
        "# chain = llm_with_tools | PydanticToolsParser(tools=[multiply, add])\n",
        "# chain.invoke(query)"
      ],
      "metadata": {
        "id": "O2ePrsEPECP_"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iQ2Rb4yRENs_"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Request: Passing tool outputs to model"
      ],
      "metadata": {
        "id": "7cr2bcCrEYk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we're using the model-generated tool invocations to actually call tools and want to pass the tool results back to the model, we can do so using ToolMessages."
      ],
      "metadata": {
        "id": "QnZHoXBGEdgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, ToolMessage\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "SavbJ1L9EtgC"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",openai_api_key=openAI_API)\n"
      ],
      "metadata": {
        "id": "Q5pCX1xHErlW"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "id": "6Mu1c5ykFFLn"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "always_multiply_llm = llm.bind_tools([multiply], tool_choice=\"multiply\")"
      ],
      "metadata": {
        "id": "aKSlsLkTFHxg"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "always_call_tool_llm = llm.bind_tools([add, multiply], tool_choice=\"any\")"
      ],
      "metadata": {
        "id": "C8EQKkCoFJat"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
        "\n",
        "llm_with_tools.invoke(query).tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2-c4AU7FLRe",
        "outputId": "7bd914a1-3b93-4893-d886-de802cad9f07"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'multiply',\n",
              "  'args': {'a': 3, 'b': 12},\n",
              "  'id': 'call_l1huJaw6zqYgT0RAtaVNJ8u2'},\n",
              " {'name': 'add',\n",
              "  'args': {'a': 11, 'b': 49},\n",
              "  'id': 'call_wGEnw99Rs7EjAGAK09FNr9tz'}]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
        "\n",
        "chain = llm_with_tools | PydanticToolsParser(tools=[multiply, add])\n",
        "chain.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FD1GC3jLFSYL",
        "outputId": "48c6b1ed-1d49-4cea-a9c2-3de1468d7a27"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[multiply(a=3, b=12), add(a=11, b=49)]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async for chunk in llm_with_tools.astream(query):\n",
        "    print(chunk.tool_call_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgFQeIJyFVLE",
        "outputId": "3f8d7d1d-ff60-4c6b-ea46-524f6e8890ef"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[{'name': 'multiply', 'args': '', 'id': 'call_SQVmQVXtZFeYiRWdzCGQ9poI', 'index': 0}]\n",
            "[{'name': None, 'args': '{\"a\"', 'id': None, 'index': 0}]\n",
            "[{'name': None, 'args': ': 3, ', 'id': None, 'index': 0}]\n",
            "[{'name': None, 'args': '\"b\": 1', 'id': None, 'index': 0}]\n",
            "[{'name': None, 'args': '2}', 'id': None, 'index': 0}]\n",
            "[{'name': 'add', 'args': '', 'id': 'call_EEV29EkCcRlLgBY2Cq64Rq6a', 'index': 1}]\n",
            "[{'name': None, 'args': '{\"a\"', 'id': None, 'index': 1}]\n",
            "[{'name': None, 'args': ': 11,', 'id': None, 'index': 1}]\n",
            "[{'name': None, 'args': ' \"b\": ', 'id': None, 'index': 1}]\n",
            "[{'name': None, 'args': '49}', 'id': None, 'index': 1}]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first = True\n",
        "async for chunk in llm_with_tools.astream(query):\n",
        "    if first:\n",
        "        gathered = chunk\n",
        "        first = False\n",
        "    else:\n",
        "        gathered = gathered + chunk\n",
        "\n",
        "    print(gathered.tool_call_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9m-uej2FZNs",
        "outputId": "70089991-9862-4c71-efea-2cb84f6f19e5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[{'name': 'multiply', 'args': '', 'id': 'call_4nEwpUZffPzfANfPHH432QMt', 'index': 0}]\n",
            "[{'name': 'multiply', 'args': '{\"a\"', 'id': 'call_4nEwpUZffPzfANfPHH432QMt', 'index': 0}]\n",
            "[{'name': 'multiply', 'args': '{\"a\": 3, ', 'id': 'call_4nEwpUZffPzfANfPHH432QMt', 'index': 0}]\n",
            "[{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 1', 'id': 'call_4nEwpUZffPzfANfPHH432QMt', 'index': 0}]\n",
            "[{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_4nEwpUZffPzfANfPHH432QMt', 'index': 0}]\n",
            "[{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_4nEwpUZffPzfANfPHH432QMt', 'index': 0}, {'name': 'add', 'args': '', 'id': 'call_vgTp6hBSuSMfOVcufbNho7xa', 'index': 1}]\n",
            "[{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_4nEwpUZffPzfANfPHH432QMt', 'index': 0}, {'name': 'add', 'args': '{\"a\"', 'id': 'call_vgTp6hBSuSMfOVcufbNho7xa', 'index': 1}]\n",
            "[{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_4nEwpUZffPzfANfPHH432QMt', 'index': 0}, {'name': 'add', 'args': '{\"a\": 11,', 'id': 'call_vgTp6hBSuSMfOVcufbNho7xa', 'index': 1}]\n",
            "[{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_4nEwpUZffPzfANfPHH432QMt', 'index': 0}, {'name': 'add', 'args': '{\"a\": 11, \"b\": ', 'id': 'call_vgTp6hBSuSMfOVcufbNho7xa', 'index': 1}]\n",
            "[{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_4nEwpUZffPzfANfPHH432QMt', 'index': 0}, {'name': 'add', 'args': '{\"a\": 11, \"b\": 49}', 'id': 'call_vgTp6hBSuSMfOVcufbNho7xa', 'index': 1}]\n",
            "[{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_4nEwpUZffPzfANfPHH432QMt', 'index': 0}, {'name': 'add', 'args': '{\"a\": 11, \"b\": 49}', 'id': 'call_vgTp6hBSuSMfOVcufbNho7xa', 'index': 1}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first = True\n",
        "async for chunk in llm_with_tools.astream(query):\n",
        "    if first:\n",
        "        gathered = chunk\n",
        "        first = False\n",
        "    else:\n",
        "        gathered = gathered + chunk\n",
        "\n",
        "    print(gathered.tool_calls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm-N9YduFePi",
        "outputId": "7d261640-6c31-4a8e-ed34-97b32b76461c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[]\n",
            "[{'name': 'multiply', 'args': {}, 'id': 'call_sh5v83H8tJgq7NJDMuVKhYZi'}]\n",
            "[{'name': 'multiply', 'args': {'a': 3}, 'id': 'call_sh5v83H8tJgq7NJDMuVKhYZi'}]\n",
            "[{'name': 'multiply', 'args': {'a': 3, 'b': 1}, 'id': 'call_sh5v83H8tJgq7NJDMuVKhYZi'}]\n",
            "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_sh5v83H8tJgq7NJDMuVKhYZi'}]\n",
            "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_sh5v83H8tJgq7NJDMuVKhYZi'}]\n",
            "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_sh5v83H8tJgq7NJDMuVKhYZi'}, {'name': 'add', 'args': {}, 'id': 'call_7hKZckWyFyaTb40sAfoK5o39'}]\n",
            "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_sh5v83H8tJgq7NJDMuVKhYZi'}, {'name': 'add', 'args': {'a': 11}, 'id': 'call_7hKZckWyFyaTb40sAfoK5o39'}]\n",
            "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_sh5v83H8tJgq7NJDMuVKhYZi'}, {'name': 'add', 'args': {'a': 11}, 'id': 'call_7hKZckWyFyaTb40sAfoK5o39'}]\n",
            "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_sh5v83H8tJgq7NJDMuVKhYZi'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_7hKZckWyFyaTb40sAfoK5o39'}]\n",
            "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_sh5v83H8tJgq7NJDMuVKhYZi'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_7hKZckWyFyaTb40sAfoK5o39'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiplies a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "tools = [add, multiply]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "messages = [HumanMessage(query)]\n",
        "ai_msg = llm_with_tools.invoke(messages)\n",
        "messages.append(ai_msg)\n",
        "\n",
        "for tool_call in ai_msg.tool_calls:\n",
        "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
        "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
        "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
        "\n",
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmXhe0TGEZOe",
        "outputId": "095fd6df-3422-46fc-b3ec-ee4030e5f043"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?'),\n",
              " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_YVUlxH4glylWzHDwn7A1tJc7', 'function': {'arguments': '{\"a\": 3, \"b\": 12}', 'name': 'multiply'}, 'type': 'function'}, {'id': 'call_k5Gmggz6U7bbiekVwOUZdXNx', 'function': {'arguments': '{\"a\": 11, \"b\": 49}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 144, 'total_tokens': 193}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-44c8fb08-bde8-46e9-84e3-6f1a717bbbf4-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_YVUlxH4glylWzHDwn7A1tJc7'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_k5Gmggz6U7bbiekVwOUZdXNx'}]),\n",
              " ToolMessage(content='36', tool_call_id='call_YVUlxH4glylWzHDwn7A1tJc7'),\n",
              " ToolMessage(content='60', tool_call_id='call_k5Gmggz6U7bbiekVwOUZdXNx')]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaUbxmuHElff",
        "outputId": "cee9b8a4-f43d-4ce3-db2b-1b8b4160033c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='3 * 12 = 36\\n11 + 49 = 60', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 209, 'total_tokens': 225}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8eeff2f4-9ed4-467f-87db-a784f1436adf-0')"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###few short prompting"
      ],
      "metadata": {
        "id": "lnkp-516Fqpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools.invoke(\n",
        "    \"Whats 119 times 8 minus 20. Don't do any math yourself, only use tools for math. Respect order of operations\"\n",
        ").tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQCKUSL1FtLA",
        "outputId": "d9cbee79-bf2f-4787-802c-bb56f50d1204"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'multiply',\n",
              "  'args': {'a': 119, 'b': 8},\n",
              "  'id': 'call_TxXzf2fIc70fbHYwUE0lhpLC'},\n",
              " {'name': 'subtract',\n",
              "  'args': {'a': 952, 'b': 20},\n",
              "  'id': 'call_L2TTjjqFER0WN5gGzB2xsibI'}]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model shouldn't be trying to add anything yet, since it technically can't know the results of 119 * 8 yet.\n",
        "\n",
        "By adding a prompt with some examples we can correct this behavior:\n",
        "\n"
      ],
      "metadata": {
        "id": "haEtgtSuF2i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "examples = [\n",
        "    HumanMessage(\n",
        "        \"What's the product of 317253 and 128472 plus four\", name=\"example_user\"\n",
        "    ),\n",
        "    AIMessage(\n",
        "        \"\",\n",
        "        name=\"example_assistant\",\n",
        "        tool_calls=[\n",
        "            {\"name\": \"multiply\", \"args\": {\"x\": 317253, \"y\": 128472}, \"id\": \"1\"}\n",
        "        ],\n",
        "    ),\n",
        "    ToolMessage(\"16505054784\", tool_call_id=\"1\"),\n",
        "    AIMessage(\n",
        "        \"\",\n",
        "        name=\"example_assistant\",\n",
        "        tool_calls=[{\"name\": \"add\", \"args\": {\"x\": 16505054784, \"y\": 4}, \"id\": \"2\"}],\n",
        "    ),\n",
        "    ToolMessage(\"16505054788\", tool_call_id=\"2\"),\n",
        "    AIMessage(\n",
        "        \"The product of 317253 and 128472 plus four is 16505054788\",\n",
        "        name=\"example_assistant\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "system = \"\"\"You are bad at math but are an expert at using a calculator.\n",
        "\n",
        "Use past tool usage as an example of how to correctly use the tools.\"\"\"\n",
        "few_shot_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        *examples,\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = {\"query\": RunnablePassthrough()} | few_shot_prompt | llm_with_tools\n",
        "chain.invoke(\"Whats 119 times 8 minus 20\").tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6JUHMWZFn4P",
        "outputId": "2709a2ff-d232-43fb-e4ef-4ce604309ae8"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'multiply',\n",
              "  'args': {'a': 119, 'b': 8},\n",
              "  'id': 'call_sCkKn9bvue9HMtH9fpKv7Qo3'}]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YMZQPA5eF6Bx"
      },
      "execution_count": 66,
      "outputs": []
    }
  ]
}